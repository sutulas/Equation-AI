{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UKUs0I8oOHOe",
        "outputId": "4736543b-0b74-4676-f109-bcb32f1c1269"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: torch==2.5.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.5.0+cu121)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (11.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch==2.5.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch==2.5.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch==2.5.0->torchvision) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tCZfS5IYr7Tr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "from torchvision.utils import save_image\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\n",
        "def quantisize(image, levels):\n",
        "    return np.digitize(image, np.arange(levels) / levels) - 1\n",
        "\n",
        "\n",
        "def str2bool(s):\n",
        "    if isinstance(s, bool):\n",
        "        return s\n",
        "    if s.lower() in ('yes', 'true', 't', 'y', '1'):\n",
        "        return True\n",
        "    elif s.lower() in ('no', 'false', 'f', 'n', '0'):\n",
        "        return False\n",
        "    else:\n",
        "        raise argparse.ArgumentTypeError('Boolean value expected')\n",
        "\n",
        "\n",
        "def nearest_square(num):\n",
        "    return round(np.sqrt(num))**2\n",
        "\n",
        "\n",
        "def save_samples(samples, dirname, filename):\n",
        "    if not os.path.exists(dirname):\n",
        "        os.mkdir(dirname)\n",
        "\n",
        "    count = samples.size()[0]\n",
        "\n",
        "    count_sqrt = int(count ** 0.5)\n",
        "    if count_sqrt ** 2 == count:\n",
        "        nrow = count_sqrt\n",
        "    else:\n",
        "        nrow = count\n",
        "\n",
        "    save_image(samples, os.path.join(dirname, filename), nrow=nrow)\n",
        "\n",
        "def duplicate_image_horizontal(image):\n",
        "    \"\"\"Duplicate the image horizontally to make it 6 times as wide.\"\"\"\n",
        "    return torch.cat([image] * 6, dim=-1)\n",
        "\n",
        "def get_loaders(dataset_name, batch_size, color_levels, train_root, test_root):\n",
        "    normalize = transforms.Lambda(lambda image: np.array(image) / 255)\n",
        "\n",
        "    discretize = transforms.Compose([\n",
        "        transforms.Lambda(lambda image: quantisize(image, color_levels)),\n",
        "        transforms.ToTensor()\n",
        "    ])\n",
        "\n",
        "    to_rgb = transforms.Compose([\n",
        "        discretize,\n",
        "        transforms.Lambda(lambda image_tensor: image_tensor.repeat(3, 1, 1)),  # Convert grayscale to RGB by repeating channels\n",
        "        transforms.Lambda(duplicate_image_horizontal)  # Apply the duplication transform\n",
        "    ])\n",
        "\n",
        "    dataset_mappings = {'mnist': 'MNIST', 'fashionmnist': 'FashionMNIST', 'cifar': 'CIFAR10'}\n",
        "    transform_mappings = {'mnist': to_rgb, 'fashionmnist': to_rgb, 'cifar': transforms.Compose([normalize, discretize])}\n",
        "    hw_mappings = {'mnist': (28, 28), 'fashionmnist': (28, 28), 'cifar': (32, 32)}\n",
        "\n",
        "    try:\n",
        "        dataset = dataset_mappings[dataset_name]\n",
        "        transform = transform_mappings[dataset_name]\n",
        "\n",
        "        train_dataset = getattr(datasets, dataset)(root=train_root, train=True, download=True, transform=transform)\n",
        "        test_dataset = getattr(datasets, dataset)(root=test_root, train=False, download=True, transform=transform)\n",
        "\n",
        "        h, w = hw_mappings[dataset_name]\n",
        "        w *= 6  # Update width to reflect the 6x duplication\n",
        "    except KeyError:\n",
        "        raise AttributeError(\"Unsupported dataset\")\n",
        "\n",
        "    print(f\"train: {train_dataset}\")\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True, drop_last=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True, drop_last=True)\n",
        "\n",
        "    return train_loader, test_loader, h, w"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/RobinXL/Handwritten-Math-Equation-Image-Generator.git\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "inLeOMs50OOy",
        "outputId": "3d1043c4-388b-4bfb-cba1-e521c51cb67e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'Handwritten-Math-Equation-Image-Generator' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Custom euqation loader\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.utils import save_image\n",
        "from PIL import Image\n",
        "import random\n",
        "\n",
        "equations = {}\n",
        "\n",
        "def quantisize(image, levels):\n",
        "    return np.digitize(image, np.arange(levels) / levels) - 1\n",
        "\n",
        "def duplicate_image_horizontal(image):\n",
        "    \"\"\"Duplicate the image horizontally to make it 6 times as wide.\"\"\"\n",
        "    return torch.cat([image] * 6, dim=-1)\n",
        "\n",
        "def generate_equation_image(digit_images, equation):\n",
        "    \"\"\"Generate a single image representing an equation, with images concatenated horizontally.\"\"\"\n",
        "    equation_image = Image.new('L', (28 * 6, 28), color=255)\n",
        "    for i, char in enumerate(equation):\n",
        "        img = random.choice(digit_images[char])  # Randomly pick an image for the digit/symbol\n",
        "        equation_image.paste(img, (i * 28, 0))  # Paste at the correct position\n",
        "    return equation_image\n",
        "\n",
        "def generate_equation_dataset(digit_images, output_dir, num_samples):\n",
        "    \"\"\"Generate images for all possible equations and save them to the output directory.\"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    num = 0\n",
        "    for i in range(1, 10):\n",
        "        for j in range(1, 10):\n",
        "            result = f\"{i + j:02d}\"  # Two-digit result format\n",
        "            equation = f\"{i}+{j}={result}\"\n",
        "            equations[equation]= num\n",
        "            num += 1\n",
        "            equation_folder = os.path.join(output_dir, equation)\n",
        "            os.makedirs(equation_folder, exist_ok=True)\n",
        "            for sample_num in range(num_samples):\n",
        "                img = generate_equation_image(digit_images, equation)\n",
        "                img.save(os.path.join(equation_folder, f\"{sample_num}.png\"))\n",
        "    print(len(equations))\n",
        "\n",
        "class EquationDataset(Dataset):\n",
        "    \"\"\"Custom dataset for loading generated equation images.\"\"\"\n",
        "    def __init__(self, data_dir, color_levels, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.color_levels = color_levels\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "\n",
        "        # Load all images and labels\n",
        "        for label_folder in os.listdir(data_dir):\n",
        "            folder_path = os.path.join(data_dir, label_folder)\n",
        "            if os.path.isdir(folder_path):\n",
        "                # Extract just the result part of the equation as a label\n",
        "                # For example, \"3x4=12\" -> 12\n",
        "                result = int(label_folder.split('=')[-1])  # Convert result to integer\n",
        "                for img_name in os.listdir(folder_path):\n",
        "                    self.image_paths.append(os.path.join(folder_path, img_name))\n",
        "                    self.labels.append(result)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image = Image.open(self.image_paths[idx]).convert(\"L\")\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label  # Returns image and numerical label\n",
        "\n",
        "\n",
        "def get_custom_loader(batch_size,color_levels,data_dir = \"/content/Handwritten-Math-Equation-Image-Generator/data\", num_train_samples=6000, num_test_samples=1000):\n",
        "    \"\"\"Generate and load the equation dataset with transformations.\"\"\"\n",
        "    # Transformations\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize((28, 28)),  # Ensure each digit image is 28x28\n",
        "        transforms.Lambda(lambda image: quantisize(np.array(image) / 255, color_levels)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Lambda(lambda image_tensor: image_tensor.repeat(3, 1, 1)),  # Convert to RGB\n",
        "        transforms.Lambda(duplicate_image_horizontal)\n",
        "    ])\n",
        "\n",
        "    # Generate datasets\n",
        "    digit_images = load_images(data_dir, (28, 28))  # Load all digit and symbol images into memory\n",
        "    train_output_dir = '/content/generated_train'\n",
        "    test_output_dir = '/content/generated_test'\n",
        "\n",
        "    generate_equation_dataset(digit_images, train_output_dir, num_train_samples)\n",
        "    generate_equation_dataset(digit_images, test_output_dir, num_test_samples)\n",
        "\n",
        "    # Load datasets\n",
        "    train_dataset = EquationDataset(train_output_dir, color_levels, transform=transform)\n",
        "    test_dataset = EquationDataset(test_output_dir, color_levels, transform=transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, drop_last=True)\n",
        "\n",
        "    return train_loader, test_loader, 28, 168\n",
        "\n",
        "def load_images(data_dir, target_size):\n",
        "    \"\"\"\n",
        "    Load images and organize by label in a dictionary.\n",
        "    Each key is a digit or symbol (0-9, x, =) and value is a list of PIL Images.\n",
        "    \"\"\"\n",
        "    image_dict = {}\n",
        "    transform = transforms.Compose([transforms.Resize(target_size), transforms.ToTensor()])\n",
        "\n",
        "    for label in os.listdir(data_dir):\n",
        "        label_path = os.path.join(data_dir, label)\n",
        "        if os.path.isdir(label_path):\n",
        "            image_dict[label] = []\n",
        "            for img_name in os.listdir(label_path):\n",
        "                img_path = os.path.join(label_path, img_name)\n",
        "                try:\n",
        "                    img = Image.open(img_path).convert(\"L\")\n",
        "                    img = transform(img)\n",
        "                    img = transforms.ToPILImage()(img)\n",
        "                    image_dict[label].append(img)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error loading image {img_path}: {e}\")\n",
        "\n",
        "    return image_dict\n",
        "\n",
        "global_train_loader, global_test_loader, global_HEIGHT, global_WIDTH = get_custom_loader(32, 2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jYevRH1Vz9g8",
        "outputId": "ebc47f61-f61f-4129-bce6-3b355e2c46d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/+/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/+/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/times/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/times/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/div/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/div/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/0/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/0/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/-/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/-/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/2/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/2/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/4/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/4/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/7/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/7/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/9/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/9/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/3/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/3/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/=/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/=/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/dot/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/dot/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/6/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/6/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/1/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/1/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/5/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/5/.DS_Store'\n",
            "Error loading image /content/Handwritten-Math-Equation-Image-Generator/data/8/.DS_Store: cannot identify image file '/content/Handwritten-Math-Equation-Image-Generator/data/8/.DS_Store'\n",
            "81\n",
            "81\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OU2A4hujshgN"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class CroppedConv2d(nn.Conv2d):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(CroppedConv2d, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = super(CroppedConv2d, self).forward(x)\n",
        "\n",
        "        kernel_height, _ = self.kernel_size\n",
        "        res = x[:, :, 1:-kernel_height, :]\n",
        "        shifted_up_res = x[:, :, :-kernel_height-1, :]\n",
        "\n",
        "        return res, shifted_up_res\n",
        "\n",
        "\n",
        "class MaskedConv2d(nn.Conv2d):\n",
        "    def __init__(self, *args, mask_type, data_channels, **kwargs):\n",
        "        super(MaskedConv2d, self).__init__(*args, **kwargs)\n",
        "\n",
        "        assert mask_type in ['A', 'B'], 'Invalid mask type.'\n",
        "\n",
        "        out_channels, in_channels, height, width = self.weight.size()\n",
        "        yc, xc = height // 2, width // 2\n",
        "\n",
        "        mask = np.zeros(self.weight.size(), dtype=np.float32)\n",
        "        mask[:, :, :yc, :] = 1\n",
        "        mask[:, :, yc, :xc + 1] = 1\n",
        "\n",
        "        def cmask(out_c, in_c):\n",
        "            a = (np.arange(out_channels) % data_channels == out_c)[:, None]\n",
        "            b = (np.arange(in_channels) % data_channels == in_c)[None, :]\n",
        "            return a * b\n",
        "\n",
        "        for o in range(data_channels):\n",
        "            for i in range(o + 1, data_channels):\n",
        "                mask[cmask(o, i), yc, xc] = 0\n",
        "\n",
        "        if mask_type == 'A':\n",
        "            for c in range(data_channels):\n",
        "                mask[cmask(c, c), yc, xc] = 0\n",
        "\n",
        "        mask = torch.from_numpy(mask).float()\n",
        "\n",
        "        self.register_buffer('mask', mask)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.weight.data *= self.mask\n",
        "        x = super(MaskedConv2d, self).forward(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v9qQ7pdMsd0l"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class CausalBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, data_channels):\n",
        "        super(CausalBlock, self).__init__()\n",
        "        self.split_size = out_channels\n",
        "\n",
        "        self.v_conv = CroppedConv2d(in_channels,\n",
        "                                    2 * out_channels,\n",
        "                                    (kernel_size // 2 + 1, kernel_size),\n",
        "                                    padding=(kernel_size // 2 + 1, kernel_size // 2))\n",
        "        self.v_fc = nn.Conv2d(in_channels,\n",
        "                              2 * out_channels,\n",
        "                              (1, 1))\n",
        "        self.v_to_h = nn.Conv2d(2 * out_channels,\n",
        "                                2 * out_channels,\n",
        "                                (1, 1))\n",
        "\n",
        "        self.h_conv = MaskedConv2d(in_channels,\n",
        "                                   2 * out_channels,\n",
        "                                   (1, kernel_size),\n",
        "                                   mask_type='A',\n",
        "                                   data_channels=data_channels,\n",
        "                                   padding=(0, kernel_size // 2))\n",
        "        self.h_fc = MaskedConv2d(out_channels,\n",
        "                                 out_channels,\n",
        "                                 (1, 1),\n",
        "                                 mask_type='A',\n",
        "                                 data_channels=data_channels)\n",
        "\n",
        "    def forward(self, image):\n",
        "        v_out, v_shifted = self.v_conv(image)\n",
        "        v_out += self.v_fc(image)\n",
        "        v_out_tanh, v_out_sigmoid = torch.split(v_out, self.split_size, dim=1)\n",
        "        v_out = torch.tanh(v_out_tanh) * torch.sigmoid(v_out_sigmoid)\n",
        "\n",
        "        h_out = self.h_conv(image)\n",
        "        v_shifted = self.v_to_h(v_shifted)\n",
        "        h_out += v_shifted\n",
        "        h_out_tanh, h_out_sigmoid = torch.split(h_out, self.split_size, dim=1)\n",
        "        h_out = torch.tanh(h_out_tanh) * torch.sigmoid(h_out_sigmoid)\n",
        "        h_out = self.h_fc(h_out)\n",
        "\n",
        "        return v_out, h_out\n",
        "\n",
        "\n",
        "class GatedBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, data_channels):\n",
        "        super(GatedBlock, self).__init__()\n",
        "        self.split_size = out_channels\n",
        "\n",
        "        self.v_conv = CroppedConv2d(in_channels,\n",
        "                                    2 * out_channels,\n",
        "                                    (kernel_size // 2 + 1, kernel_size),\n",
        "                                    padding=(kernel_size // 2 + 1, kernel_size // 2))\n",
        "        self.v_fc = nn.Conv2d(in_channels,\n",
        "                              2 * out_channels,\n",
        "                              (1, 1))\n",
        "        self.v_to_h = MaskedConv2d(2 * out_channels,\n",
        "                                   2 * out_channels,\n",
        "                                   (1, 1),\n",
        "                                   mask_type='B',\n",
        "                                   data_channels=data_channels)\n",
        "\n",
        "        self.h_conv = MaskedConv2d(in_channels,\n",
        "                                   2 * out_channels,\n",
        "                                   (1, kernel_size),\n",
        "                                   mask_type='B',\n",
        "                                   data_channels=data_channels,\n",
        "                                   padding=(0, kernel_size // 2))\n",
        "        self.h_fc = MaskedConv2d(out_channels,\n",
        "                                 out_channels,\n",
        "                                 (1, 1),\n",
        "                                 mask_type='B',\n",
        "                                 data_channels=data_channels)\n",
        "\n",
        "        self.h_skip = MaskedConv2d(out_channels,\n",
        "                                   out_channels,\n",
        "                                   (1, 1),\n",
        "                                   mask_type='B',\n",
        "                                   data_channels=data_channels)\n",
        "\n",
        "        self.label_embedding = nn.Embedding(81, 2*out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        v_in, h_in, skip, label = x[0], x[1], x[2], x[3]\n",
        "\n",
        "        label_embedded = self.label_embedding(label).unsqueeze(2).unsqueeze(3)\n",
        "\n",
        "        v_out, v_shifted = self.v_conv(v_in)\n",
        "        v_out += self.v_fc(v_in)\n",
        "        v_out += label_embedded\n",
        "        v_out_tanh, v_out_sigmoid = torch.split(v_out, self.split_size, dim=1)\n",
        "        v_out = torch.tanh(v_out_tanh) * torch.sigmoid(v_out_sigmoid)\n",
        "\n",
        "        h_out = self.h_conv(h_in)\n",
        "        v_shifted = self.v_to_h(v_shifted)\n",
        "        h_out += v_shifted\n",
        "        h_out += label_embedded\n",
        "        h_out_tanh, h_out_sigmoid = torch.split(h_out, self.split_size, dim=1)\n",
        "        h_out = torch.tanh(h_out_tanh) * torch.sigmoid(h_out_sigmoid)\n",
        "\n",
        "        # skip connection\n",
        "        skip = skip + self.h_skip(h_out)\n",
        "\n",
        "        h_out = self.h_fc(h_out)\n",
        "\n",
        "        # residual connections\n",
        "        h_out = h_out + h_in\n",
        "        v_out = v_out + v_in\n",
        "\n",
        "        return {0: v_out, 1: h_out, 2: skip, 3: label}\n",
        "\n",
        "\n",
        "class PixelCNN(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super(PixelCNN, self).__init__()\n",
        "\n",
        "        DATA_CHANNELS = 3\n",
        "\n",
        "        self.hidden_fmaps = cfg[\"hidden_fmaps\"]\n",
        "        self.color_levels = cfg[\"color_levels\"]\n",
        "\n",
        "        self.causal_conv = CausalBlock(DATA_CHANNELS,\n",
        "                                       cfg[\"hidden_fmaps\"],\n",
        "                                       cfg[\"causal_ksize\"],\n",
        "                                       data_channels=DATA_CHANNELS)\n",
        "\n",
        "        self.hidden_conv = nn.Sequential(\n",
        "            *[GatedBlock(cfg[\"hidden_fmaps\"], cfg[\"hidden_fmaps\"], cfg[\"hidden_ksize\"], DATA_CHANNELS) for _ in range(cfg[\"hidden_layers\"])]\n",
        "        )\n",
        "\n",
        "        size = len(equations)\n",
        "        self.label_embedding = nn.Embedding(81, self.hidden_fmaps)\n",
        "        # self.vocabulary = set()\n",
        "        # for equation in equations:\n",
        "        #     self.vocabulary.update(tokenize_equation(equation))\n",
        "        # self.vocabulary = list(self.vocabulary)\n",
        "        # self.token_to_index = {token: index for index, token in enumerate(self.vocabulary)}\n",
        "        # self.embedding_layer = torch.nn.Embedding(len(self.vocabulary), 4)\n",
        "\n",
        "\n",
        "        self.out_hidden_conv = MaskedConv2d(cfg[\"hidden_fmaps\"],\n",
        "                                            cfg[\"out_hidden_fmaps\"],\n",
        "                                            (1, 1),\n",
        "                                            mask_type='B',\n",
        "                                            data_channels=DATA_CHANNELS)\n",
        "\n",
        "        self.out_conv = MaskedConv2d(cfg[\"out_hidden_fmaps\"],\n",
        "                                     DATA_CHANNELS * cfg[\"color_levels\"],\n",
        "                                     (1, 1),\n",
        "                                     mask_type='B',\n",
        "                                     data_channels=DATA_CHANNELS)\n",
        "\n",
        "    def forward(self, image, label):\n",
        "        count, data_channels, height, width = image.size()\n",
        "\n",
        "        v, h = self.causal_conv(image)\n",
        "\n",
        "        _, _, out, _ = self.hidden_conv({0: v,\n",
        "                                         1: h,\n",
        "                                         2: image.new_zeros((count, self.hidden_fmaps, height, width), requires_grad=True),\n",
        "                                         3: label}).values()\n",
        "\n",
        "        # label = equations[label]\n",
        "        # label = torch.tensor([label]).to(image.device)\n",
        "        label_embedded = self.label_embedding(label).unsqueeze(2).unsqueeze(3)\n",
        "        # tokens = tokenize_equation(label)\n",
        "        # token_indices = [self.token_to_index[token] for token in tokens]\n",
        "        # embedded_equation = self.embedding_layer(torch.tensor(token_indices))\n",
        "\n",
        "\n",
        "        # add label bias\n",
        "        out += label_embedded\n",
        "        out = F.relu(out)\n",
        "        out = F.relu(self.out_hidden_conv(out))\n",
        "        out = self.out_conv(out)\n",
        "\n",
        "        out = out.view(count, self.color_levels, data_channels, height, width)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def sample(self, shape, count, label=None, device='cuda'):\n",
        "        channels, height, width = shape\n",
        "\n",
        "        samples = torch.zeros(count, *shape).to(device)\n",
        "        if label is None:\n",
        "            labels = torch.randint(high=10, size=(count,)).to(device)\n",
        "        else:\n",
        "            labels = (label*torch.ones(count)).to(device).long()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i in range(height):\n",
        "                for j in range(width):\n",
        "                    for c in range(channels):\n",
        "                        unnormalized_probs = self.forward(samples, labels)\n",
        "                        pixel_probs = torch.softmax(unnormalized_probs[:, :, c, i, j], dim=1)\n",
        "                        sampled_levels = torch.multinomial(pixel_probs, 1).squeeze().float() / (self.color_levels - 1)\n",
        "                        samples[:, c, i, j] = sampled_levels\n",
        "\n",
        "        return samples\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TDStTDYEr98J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "946a1001-84af-4a59-8041-2a64c2ee6e86"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:ts2sf6mn) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <style>\n",
              "        .wandb-row {\n",
              "            display: flex;\n",
              "            flex-direction: row;\n",
              "            flex-wrap: wrap;\n",
              "            justify-content: flex-start;\n",
              "            width: 100%;\n",
              "        }\n",
              "        .wandb-col {\n",
              "            display: flex;\n",
              "            flex-direction: column;\n",
              "            flex-basis: 100%;\n",
              "            flex: 1;\n",
              "            padding: 10px;\n",
              "        }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test loss</td><td>█▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test loss</td><td>0.00239</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sweet-wildflower-41</strong> at: <a href='https://wandb.ai/sutulaseamus-boston-college/PixelCNN/runs/ts2sf6mn' target=\"_blank\">https://wandb.ai/sutulaseamus-boston-college/PixelCNN/runs/ts2sf6mn</a><br/> View project at: <a href='https://wandb.ai/sutulaseamus-boston-college/PixelCNN' target=\"_blank\">https://wandb.ai/sutulaseamus-boston-college/PixelCNN</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20241114_161333-ts2sf6mn/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:ts2sf6mn). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.6"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20241114_214754-shemv5m3</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sutulaseamus-boston-college/PixelCNN/runs/shemv5m3' target=\"_blank\">apricot-tree-42</a></strong> to <a href='https://wandb.ai/sutulaseamus-boston-college/PixelCNN' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/sutulaseamus-boston-college/PixelCNN' target=\"_blank\">https://wandb.ai/sutulaseamus-boston-college/PixelCNN</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/sutulaseamus-boston-college/PixelCNN/runs/shemv5m3' target=\"_blank\">https://wandb.ai/sutulaseamus-boston-college/PixelCNN/runs/shemv5m3</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/15: 100%|██████████| 15187/15187 [1:05:17<00:00,  3.88it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Average test loss: 0.0025502557400614023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/15:  60%|██████    | 9167/15187 [39:27<25:57,  3.87it/s]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "import numpy as np\n",
        "\n",
        "import argparse\n",
        "import os\n",
        "\n",
        "from tqdm import tqdm\n",
        "import wandb\n",
        "\n",
        "\n",
        "TRAIN_DATASET_ROOT = '.data/train/'\n",
        "TEST_DATASET_ROOT = '.data/test/'\n",
        "\n",
        "MODEL_PARAMS_OUTPUT_DIR = 'model'\n",
        "MODEL_PARAMS_OUTPUT_FILENAME = 'params.pth'\n",
        "\n",
        "TRAIN_SAMPLES_DIR = 'train_samples'\n",
        "\n",
        "import re\n",
        "\n",
        "# def tokenize_equation(equation):\n",
        "#     \"\"\"Tokenizes an equation string into individual tokens of numbers and operators.\"\"\"\n",
        "#     # Split the equation into tokens (numbers, operators, etc.)\n",
        "#     tokens = re.findall(r'\\d+|\\+|=|-|\\*|\\/', equation)\n",
        "#     return tokens\n",
        "\n",
        "def train(cfg, model, device, train_loader, optimizer, scheduler, epoch):\n",
        "    model.train()\n",
        "\n",
        "    for images, labels in tqdm(train_loader, desc='Epoch {}/{}'.format(epoch + 1, cfg[\"epochs\"])):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "        normalized_images = images.float() / (cfg[\"color_levels\"] - 1)\n",
        "\n",
        "\n",
        "        outputs = model(normalized_images, labels)\n",
        "        loss = F.cross_entropy(outputs, images)\n",
        "        loss.backward()\n",
        "\n",
        "        clip_grad_norm_(model.parameters(), max_norm=cfg[\"max_norm\"])\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "\n",
        "def test_and_sample(cfg, model, device, test_loader, height, width, losses, params, epoch):\n",
        "    test_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device, non_blocking=True)\n",
        "            labels = labels.to(device, non_blocking=True)\n",
        "\n",
        "            normalized_images = images.float() / (cfg[\"color_levels\"] - 1)\n",
        "            outputs = model(normalized_images, labels)\n",
        "\n",
        "            test_loss += F.cross_entropy(outputs, images, reduction='none')\n",
        "\n",
        "    test_loss = test_loss.mean().cpu() / len(test_loader.dataset)\n",
        "\n",
        "    wandb.log({\n",
        "        \"Test loss\": test_loss\n",
        "    })\n",
        "    print(\"Average test loss: {}\".format(test_loss))\n",
        "\n",
        "    losses.append(test_loss)\n",
        "    params.append(model.state_dict())\n",
        "\n",
        "    samples = model.sample((3, height, width), cfg[\"epoch_samples\"], device=device)\n",
        "    save_samples(samples, TRAIN_SAMPLES_DIR, 'epoch{}_samples.png'.format(epoch + 1))\n",
        "\n",
        "\n",
        "# Define a global variable to store the saved model path\n",
        "SAVED_MODEL_PATH = None\n",
        "\n",
        "def main(\n",
        "    epochs=15,\n",
        "    batch_size=32,\n",
        "    dataset='mnist',\n",
        "    causal_ksize=7,\n",
        "    hidden_ksize=7,\n",
        "    color_levels=2,\n",
        "    hidden_fmaps=30,\n",
        "    out_hidden_fmaps=10,\n",
        "    hidden_layers=6,\n",
        "    learning_rate=0.0001,\n",
        "    weight_decay=0.0001,\n",
        "    max_norm=1.0,\n",
        "    epoch_samples=3,\n",
        "    cuda=True,\n",
        "    model_path=None,\n",
        "    output_fname='samples.png',\n",
        "    label=-1,\n",
        "    count=64,\n",
        "    height=28,\n",
        "    width=168\n",
        "):\n",
        "    global SAVED_MODEL_PATH  # Access the global variable\n",
        "\n",
        "    # Configuration dictionary\n",
        "    cfg = {\n",
        "        \"epochs\": epochs,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"dataset\": dataset,\n",
        "        \"causal_ksize\": causal_ksize,\n",
        "        \"hidden_ksize\": hidden_ksize,\n",
        "        \"color_levels\": color_levels,\n",
        "        \"hidden_fmaps\": hidden_fmaps,\n",
        "        \"out_hidden_fmaps\": out_hidden_fmaps,\n",
        "        \"hidden_layers\": hidden_layers,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"max_norm\": max_norm,\n",
        "        \"epoch_samples\": epoch_samples,\n",
        "        \"cuda\": cuda\n",
        "    }\n",
        "\n",
        "    if model_path is None:\n",
        "        # Training and saving model\n",
        "        wandb.init(project=\"PixelCNN\")\n",
        "        wandb.config.update(cfg)\n",
        "        torch.manual_seed(42)\n",
        "\n",
        "        model = PixelCNN(cfg=cfg)\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and cfg[\"cuda\"] else \"cpu\")\n",
        "        model.to(device)\n",
        "\n",
        "        # train_loader, test_loader, HEIGHT, WIDTH = get_loaders(\n",
        "        #     cfg[\"dataset\"], cfg[\"batch_size\"], cfg[\"color_levels\"], TRAIN_DATASET_ROOT, TEST_DATASET_ROOT\n",
        "        # )\n",
        "        train_loader, test_loader, HEIGHT, WIDTH = global_train_loader, global_test_loader, global_HEIGHT, global_WIDTH\n",
        "\n",
        "\n",
        "        optimizer = optim.Adam(model.parameters(), lr=cfg[\"learning_rate\"], weight_decay=cfg[\"weight_decay\"])\n",
        "        scheduler = optim.lr_scheduler.CyclicLR(optimizer, cfg[\"learning_rate\"], 10 * cfg[\"learning_rate\"], cycle_momentum=False)\n",
        "\n",
        "        wandb.watch(model)\n",
        "\n",
        "        losses = []\n",
        "        params = []\n",
        "\n",
        "        for epoch in range(cfg[\"epochs\"]):\n",
        "            train(cfg, model, device, train_loader, optimizer, scheduler, epoch)\n",
        "            test_and_sample(cfg, model, device, test_loader, HEIGHT, WIDTH, losses, params, epoch)\n",
        "\n",
        "        # Save model parameters\n",
        "        if not os.path.exists(MODEL_PARAMS_OUTPUT_DIR):\n",
        "            os.mkdir(MODEL_PARAMS_OUTPUT_DIR)\n",
        "        MODEL_PARAMS_OUTPUT_FILENAME = '{}_cks{}hks{}cl{}hfm{}ohfm{}hl{}_params.pth'.format(\n",
        "            cfg[\"dataset\"], cfg[\"causal_ksize\"], cfg[\"hidden_ksize\"], cfg[\"color_levels\"], cfg[\"hidden_fmaps\"],\n",
        "            cfg[\"out_hidden_fmaps\"], cfg[\"hidden_layers\"]\n",
        "        )\n",
        "        SAVED_MODEL_PATH = os.path.join(MODEL_PARAMS_OUTPUT_DIR, MODEL_PARAMS_OUTPUT_FILENAME)\n",
        "        torch.save(params[np.argmin(np.array(losses))], SAVED_MODEL_PATH)\n",
        "        print(f\"Model saved to {SAVED_MODEL_PATH}\")\n",
        "    else:\n",
        "        # Testing and loading the model\n",
        "        OUTPUT_FILENAME = output_fname\n",
        "        model = PixelCNN(cfg=cfg)\n",
        "        model.eval()\n",
        "\n",
        "        device = torch.device(\"cuda\" if torch.cuda.is_available() and cfg[\"cuda\"] else \"cpu\")\n",
        "        model.to(device)\n",
        "\n",
        "        model.load_state_dict(torch.load(model_path))\n",
        "\n",
        "        label = None if label == -1 else label\n",
        "        samples = model.sample((3, height, width), count, label=label, device=device)\n",
        "        save_samples(samples, TRAIN_SAMPLES_DIR, OUTPUT_FILENAME)\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4AvBPdwSiu6n"
      },
      "outputs": [],
      "source": [
        "SAVED_MODEL_PATH = \"model/mnist_cks7hks7cl2hfm30ohfm10hl6_params.pth\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TEWQ61YBRw5x"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "import argparse\n",
        "\n",
        "OUTPUT_DIRNAME = 'samples'\n",
        "\n",
        "\n",
        "import torch\n",
        "\n",
        "def main(\n",
        "    causal_ksize=7,\n",
        "    hidden_ksize=7,\n",
        "    color_levels=2,\n",
        "    hidden_fmaps=30,\n",
        "    out_hidden_fmaps=10,\n",
        "    hidden_layers=6,\n",
        "    cuda=True,\n",
        "    model_path=None,\n",
        "    output_fname='samples.png',\n",
        "    label=-1,\n",
        "    count=64,\n",
        "    height=28,\n",
        "    width=168\n",
        "):\n",
        "    # Configuration dictionary\n",
        "    cfg = {\n",
        "        \"causal_ksize\": causal_ksize,\n",
        "        \"hidden_ksize\": hidden_ksize,\n",
        "        \"color_levels\": color_levels,\n",
        "        \"hidden_fmaps\": hidden_fmaps,\n",
        "        \"out_hidden_fmaps\": out_hidden_fmaps,\n",
        "        \"hidden_layers\": hidden_layers,\n",
        "        \"cuda\": cuda,\n",
        "        \"model_path\": model_path,\n",
        "        \"output_fname\": output_fname,\n",
        "        \"label\": label,\n",
        "        \"count\": count,\n",
        "        \"height\": height,\n",
        "        \"width\": width\n",
        "    }\n",
        "\n",
        "    if cfg[\"model_path\"] is None:\n",
        "        raise ValueError(\"model_path must be specified to load the model.\")\n",
        "\n",
        "    OUTPUT_FILENAME = cfg[\"output_fname\"]\n",
        "\n",
        "    model = PixelCNN(cfg=cfg)\n",
        "    model.eval()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and cfg[\"cuda\"] else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Loading the model with `weights_only=True` as a safer default\n",
        "    model.load_state_dict(torch.load(cfg[\"model_path\"], weights_only=True))\n",
        "\n",
        "    label = None if cfg[\"label\"] == -1 else cfg[\"label\"]\n",
        "    samples = model.sample((3, cfg[\"height\"], cfg[\"width\"]), cfg[\"count\"], label=label, device=device)\n",
        "    save_samples(samples, OUTPUT_DIRNAME, OUTPUT_FILENAME)\n",
        "\n",
        "\n",
        "\n",
        "main(model_path=SAVED_MODEL_PATH)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "caUqw-vhfb_b"
      },
      "outputs": [],
      "source": [
        "print(SAVED_MODEL_PATH)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aicB20vxfPSV"
      },
      "outputs": [],
      "source": [
        "if __name__ == '__main__':\n",
        "    # Define the path where the model is saved\n",
        "\n",
        "    # Ask the user for a single digit\n",
        "    digit_input = input(\"Enter a single digit (0-9) to generate its image: \")\n",
        "\n",
        "    # Ensure the input is valid\n",
        "    try:\n",
        "        label = int(digit_input)\n",
        "        if label < 0 or label > 9:\n",
        "            raise ValueError(\"Please enter a single digit between 0 and 9.\")\n",
        "    except ValueError as e:\n",
        "        print(e)\n",
        "        exit(1)\n",
        "    print(SAVED_MODEL_PATH)\n",
        "    # Configuration dictionary for loading model and generating samples\n",
        "    cfg = {\n",
        "        \"causal_ksize\": 7,\n",
        "        \"hidden_ksize\": 7,\n",
        "        \"color_levels\": 2,\n",
        "        \"hidden_fmaps\": 30,\n",
        "        \"out_hidden_fmaps\": 10,\n",
        "        \"hidden_layers\": 6,\n",
        "        \"cuda\": True,\n",
        "        \"model_path\": SAVED_MODEL_PATH,\n",
        "        \"output_fname\": f'digit_{label}_sample.png',\n",
        "        \"label\": label,\n",
        "        \"count\": 1,  # Generate a single image for the digit\n",
        "        \"height\": 28,\n",
        "        \"width\": 168\n",
        "    }\n",
        "\n",
        "    # Initialize the model\n",
        "    model = PixelCNN(cfg=cfg)\n",
        "    model.eval()\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() and cfg[\"cuda\"] else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    # Load the pre-trained model weights\n",
        "    model.load_state_dict(torch.load(cfg[\"model_path\"]))\n",
        "\n",
        "    # Generate and save the sample for the specified digit\n",
        "    samples = model.sample((3, cfg[\"height\"], cfg[\"width\"]), cfg[\"count\"], label=label, device=device)\n",
        "    save_samples(samples, OUTPUT_DIRNAME, cfg[\"output_fname\"])\n",
        "\n",
        "    print(f\"Image of digit {label} generated and saved as {cfg['output_fname']}.\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}